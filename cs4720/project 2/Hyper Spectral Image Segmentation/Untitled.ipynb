{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib as matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from scipy.io import loadmat\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy import exp, signal\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import (manifold, datasets, metrics, decomposition, cluster, ensemble,discriminant_analysis, random_projection)\n",
    "from sklearn.neighbors import kneighbors_graph, KNeighborsClassifier\n",
    "from sklearn import mixture\n",
    "import scipy.misc as scimisc\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.special import comb\n",
    "from scipy.spatial import distance\n",
    "import sklearn.decomposition as dec\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "def contingency_matrix(labels_true, labels_pred, eps=None):\n",
    "    classes, class_idx = np.unique(labels_true, return_inverse=True)\n",
    "    clusters, cluster_idx = np.unique(labels_pred, return_inverse=True)\n",
    "    n_classes = classes.shape[0]\n",
    "    n_clusters = clusters.shape[0]\n",
    "    # Using coo_matrix to accelerate simple histogram calculation,\n",
    "    # i.e. bins are consecutive integers\n",
    "    # Currently, coo_matrix is faster than histogram2d for simple cases\n",
    "    contingency = coo_matrix((np.ones(class_idx.shape[0]),\n",
    "                             (class_idx, cluster_idx)),\n",
    "                            shape=(n_classes, n_clusters),\n",
    "                            dtype=np.int).toarray()\n",
    "    if eps is not None:\n",
    "        # don't use += as contingency is integer\n",
    "        contingency = contingency + eps\n",
    "    return contingency\n",
    "\n",
    "def check_clusterings(labels_true, labels_pred):\n",
    "    \"\"\"Check that the two clusterings matching 1D integer arrays\"\"\"\n",
    "    labels_true = np.asarray(labels_true)\n",
    "    labels_pred = np.asarray(labels_pred)\n",
    "\n",
    "    # input checks\n",
    "    if labels_true.ndim != 1:\n",
    "        raise ValueError(\n",
    "            \"labels_true must be 1D: shape is %r\" % (labels_true.shape,))\n",
    "    if labels_pred.ndim != 1:\n",
    "        raise ValueError(\n",
    "            \"labels_pred must be 1D: shape is %r\" % (labels_pred.shape,))\n",
    "    if labels_true.shape != labels_pred.shape:\n",
    "        raise ValueError(\n",
    "            \"labels_true and labels_pred must have same size, got %d and %d\"\n",
    "            % (labels_true.shape[0], labels_pred.shape[0]))\n",
    "    return labels_true, labels_pred\n",
    "\n",
    "def rand_score(labels_true, labels_pred):\n",
    "   \n",
    "    labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n",
    "    n_samples = labels_true.shape[0]\n",
    "    classes = np.unique(labels_true)\n",
    "    clusters = np.unique(labels_pred)\n",
    "    # Special limit cases: no clustering since the data is not split;\n",
    "    # or trivial clustering where each document is assigned a unique cluster.\n",
    "    # These are perfect matches hence return 1.0.\n",
    "    if (classes.shape[0] == clusters.shape[0] == 1\n",
    "            or classes.shape[0] == clusters.shape[0] == 0\n",
    "            or classes.shape[0] == clusters.shape[0] == len(labels_true)):\n",
    "        return 1.0\n",
    "    \n",
    "    contingency = contingency_matrix(labels_true, labels_pred)\n",
    "\n",
    "    # Compute the ARI using the contingency data\n",
    "    sum_comb_c = sum(comb2(n_c) for n_c in contingency.sum(axis=1))\n",
    "    sum_comb_k = sum(comb2(n_k) for n_k in contingency.sum(axis=0))\n",
    "    \n",
    "    sum_comb = sum(comb2(n_ij) for n_ij in contingency.flatten())\n",
    "    t_p=sum_comb\n",
    "    f_p=sum_comb_c-sum_comb\n",
    "    f_n=sum_comb_k-sum_comb\n",
    "    t_n=float(comb(n_samples, 2))-t_p-f_p-f_n\n",
    "    result=(t_n+t_p)/float(comb(n_samples, 2))\n",
    "    return result\n",
    "\n",
    "def comb2(n):\n",
    "    # the exact version is faster for k == 2: use it by default globally in\n",
    "    # this module instead of the float approximate variant\n",
    "    return comb(n, 2, exact=1)\n",
    "\n",
    "def randindex(ground_truth,clust_result):\n",
    "    a=0\n",
    "    b=0\n",
    "    n=len(ground_truth)\n",
    "    for i in range(0,n-1):\n",
    "        for j in range(i+1,n):\n",
    "            a= a+int(((ground_truth[i]==ground_truth[j]))and(clust_result[i]==clust_result[j]))\n",
    "            b= b+int(((ground_truth[i]!=ground_truth[j]))and(clust_result[i]!=clust_result[j]))\n",
    "   \n",
    "    rand_index=(200*(a+b))/(n*(n-1))\n",
    "    return rand_index\n",
    "\n",
    "def removeBackground(image_flat, groundTruth_flat):\n",
    "\n",
    "    indices = np.where(groundTruth_flat==0)\n",
    "    \n",
    "    image_flat = np.delete(image_flat,indices)\n",
    "    groundTruth_flat = np.delete(groundTruth_flat,indices)\n",
    "\n",
    "    return image_flat,groundTruth_flat\n",
    "\n",
    "def smartPCA(flatImage, bandwidth, n_components=1):\n",
    "\n",
    "    dim = flatImage.shape[1]\n",
    "\n",
    "    numBands = int(dim/bandwidth)\n",
    "    extras = np.mod(dim,bandwidth)\n",
    "\n",
    "    whitenedData = np.zeros((flatImage.shape[0],numBands))\n",
    "\n",
    "    startCounter = 0\n",
    "    endCounter = bandwidth\n",
    "    pca = dec.PCA(n_components=n_components,whiten=True)\n",
    "    for i in range(0,numBands):\n",
    "\n",
    "        if extras:\n",
    "            endCounter=endCounter+1\n",
    "            extras = extras - 1\n",
    "\n",
    "        sectionToWhiten = flatImage[:,startCounter:endCounter]\n",
    "        whiten = pca.fit_transform(sectionToWhiten)\n",
    "        startCounter = startCounter + bandwidth\n",
    "        endCounter = endCounter + bandwidth\n",
    "        whitenedData[:,i] = whiten.T\n",
    "\n",
    "\n",
    "    return whitenedData\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gt = loadmat('/home/lagaunne/Desktop/Indian_pines_gt.mat')\n",
    "    gt=gt['indian_pines_gt']\n",
    "    data = loadmat('/home/lagaunne/Desktop/Indian_pines_corrected.mat')\n",
    "    data=data['indian_pines_corrected']\n",
    "    sc  = StandardScaler()\n",
    "    data_bg = sc.fit_transform(np.reshape(data, [145*145, 200]))\n",
    "    \n",
    "#Feature Extraction\n",
    "#     nDim = 4\n",
    "    \n",
    "#     data_bg1 = data[0:70,0:70,:]\n",
    "#     data_bg2 = data[0:70,70:145,:]\n",
    "#     data_bg3 = data[70:145,0:70,:]\n",
    "#     data_bg4 = data[70:145,70:145,:]\n",
    "     \n",
    "#     pca_data1 = smartPCA(sc.fit_transform(np.reshape(data_bg1, (70*70,200))), 200/nDim)\n",
    "#     pca_data1 = np.reshape(pca_data1, (70, 70, nDim))\n",
    "#     pca_data2 = smartPCA(sc.fit_transform(np.reshape(data_bg2, (70*75,200))), 200/nDim)\n",
    "#     pca_data2 = np.reshape(pca_data2, (70, 75, nDim))\n",
    "#     pca_data1 = np.append(pca_data1, pca_data2, axis=1)    \n",
    "     \n",
    "#     pca_data3 = smartPCA(sc.fit_transform(np.reshape(data_bg3, (75*70,200))), 200/nDim)\n",
    "#     pca_data3 = np.reshape(pca_data3, (75, 70,nDim))\n",
    "#     pca_data4 = smartPCA(sc.fit_transform(np.reshape(data_bg4, (75*75,200))), 200/nDim)\n",
    "#     pca_data4 = np.reshape(pca_data4, (75, 75,nDim))\n",
    "#     pca_data3 = np.append(pca_data3, pca_data4, axis=1)\n",
    "     \n",
    "#     pca_data = np.append(pca_data1, pca_data3, axis=0)\n",
    "\n",
    "#     print(pca_data.shape)\n",
    "    \n",
    "#     pca_data = np.append(pca_data, np.reshape(smartPCA(data_bg, 200/4), (145,145,4)), axis=2)\n",
    "#     pca_data = np.reshape(pca_data, (145*145, nDim+4))    \n",
    "#     pca_data = sc.fit_transform(pca_data)\n",
    "    \n",
    "    #X=pca_data\n",
    "    #Local_Split_PCA\n",
    "    #X=np.load('/home/lagaunne/project-2-team/Local_Split_PCA.npy')\n",
    "    #Split_PCA\n",
    "    #X=np.load('/home/lagaunne/project-2-team/Split_PCA.npy')\n",
    "    #Global_PCA\n",
    "    #X=np.load('/home/lagaunne/project-2-team/GPCA.npy')\n",
    "    #Local_PCA\n",
    "    X=np.load('/home/lagaunne/project-2-team/LPCA.npy')\n",
    "    X=np.reshape(X,[145*145,4])\n",
    "    X= sc.fit_transform(X)\n",
    "    K_means = cluster.KMeans(n_clusters=100)\n",
    "    K_means.fit(X)\n",
    "    words = K_means.labels_.astype(np.int)\n",
    "    K_means2 = cluster.KMeans(n_clusters=17)\n",
    "    K_means2.fit(X)\n",
    "    document = K_means2.labels_.astype(np.int)\n",
    "    document=np.array(document)\n",
    "\n",
    "    feature=contingency_matrix(document,words)\n",
    "\n",
    "    model = lda.LDA(n_topics=17, n_iter=2000, random_state=1)\n",
    "    model.fit(feature)\n",
    "    topic_word = model.topic_word_  # model.components_ also works\n",
    "    sp=topic_word.shape\n",
    "\n",
    "    topic_label=[]\n",
    "    for i in range(0,sp[1]):\n",
    "        index=0\n",
    "        value=0\n",
    "        for j in  range(0,sp[0]):\n",
    "            if topic_word[j,i]>value:\n",
    "                value=topic_word[j,i]\n",
    "                index=j\n",
    "        topic_label.append(index)\n",
    "\n",
    "    image_label=[]\n",
    "    for i in range(0,len(words)):\n",
    "        for j in  range(0,len(topic_label)):\n",
    "            if words[i]==j:\n",
    "                image_label.append(topic_label[j])\n",
    "\n",
    "    words_new=np.reshape(words,(145,145))\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(words_new)\n",
    "\n",
    "    document_new=np.reshape(document,(145,145))\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(document_new)\n",
    "\n",
    "    image_label=np.array(image_label)\n",
    "    LDA_result=np.reshape(image_label,(145,145))\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(LDA_result)\n",
    "       \n",
    "         \n",
    "    #Rand Score\n",
    "    newLabels, newGt    = removeBackground(np.reshape(LDA_result, [145*145]), np.reshape(gt, [145*145]))\n",
    "\n",
    "    percentage = rand_score(newGt, newLabels)\n",
    "    print(percentage)\n",
    "    plt.imsave('/home/lagaunne/project-2-team/LDA_Result/LPCA_Result.jpg',LDA_result)\n",
    "    np.save('/home/lagaunne/project-2-team/LDA_Result/LPCA_Result.npy',image_label)\n",
    "    #plt.imshow(LDA_result)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
